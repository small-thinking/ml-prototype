seed_everything: 42
trainer:
  max_epochs: 3
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: ./lightning_logs
      name: feedforward-c16-b128-rmsn
  callbacks:
    class_path: lm.model.DummyCallback

model:
  class_path: lm.model.Seq2SeqLM
  init_args:
    model:
      class_path: lm.module.FeedForwardLM
      init_args:
        config:
          context_size: &context_size 16
          vocab_size: &vocab_size 65
          has_embedding: true
          input_dim: 64
          layer_sizes: [128, 128]
          pre_norm: true
          norm_type: rms_norm
    loss:
      class_path: torch.nn.CrossEntropyLoss
    vocab_size: *vocab_size
    
data:
  class_path: lm.data_module.InMemoryDataModule
  init_args:
    config:
      data_path: ~/Downloads/test_data.txt
      samples_per_epoch: 100000
      batch_size: 128
      context_size: *context_size

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.001
    weight_decay: 0.01

  
