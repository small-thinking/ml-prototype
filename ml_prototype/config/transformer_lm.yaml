seed_everything: null
trainer:
  precision: 16-mixed
  devices: 1
  # strategy: ddp
  max_epochs: 20
  accumulate_grad_batches: 1
  gradient_clip_val: null
  # logger:
  #   - class_path: lightning.pytorch.loggers.WandbLogger
  #     init_args:
  #       project: dummy_model
  #       # name: &name books1-s-bpe5000-ln-lp-d512-h8l10-lr1e-4
  #       name: &name philosopher-bpe256-ln-lp-d512-h4l4

  callbacks:
    - class_path: lm.model.DummyCallback
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        # dirpath: ./checkpoints/books1-s-bpe5000-ln-lp-d512-h8l10-lr1e-4
        dirpath: ./checkpoints/philosopher-bpe256-ln-lp-d512-h4l4
        every_n_epochs: 2
        filename: "{epoch}-{val_loss:.2f}"
        verbose: true
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_loss
        min_delta: 0.002
        patience: 3
    - class_path: lm.model.TorchScriptCallback
      init_args:
        config:
          batch_size: &batch_size 32
          seq_len: &seq_len 512
          embed_dim: &embed_dim 512
          num_heads: &num_heads 4
          num_layers: &num_layers 4
          dropout_ratio: &dropout_ratio 0.1
          activation_type: &activation_type relu

          pos_embedding_type: &pos_embedding_type simple

          vocab_size: &vocab_size 256  # for bpe/spm tokenizer
          # vocab_size: &vocab_size 216  # for char tokenizer
          save_every_epoch: 5
    - class_path: lightning.pytorch.callbacks.GradientAccumulationScheduler
      init_args:
        scheduling:
          0: 1
          1: 5
          5: 1

model:
  class_path: lm.model.Seq2SeqLM
  init_args:
    model:
      class_path: lm.module.TransformerLM
      init_args:
        config:
          vocab_size: *vocab_size
          seq_len: *seq_len
          embed_dim: *embed_dim
          num_heads: *num_heads
          num_layers: *num_layers
          batch_first: true
          norm_first: true
          dropout_ratio: *dropout_ratio
          pos_embedding_type: *pos_embedding_type
          vocab_size: *vocab_size
          activation_type: *activation_type
          use_custom_attention: false
          use_customized_scaled_doc_product_attention: false

    loss:
      class_path: torch.nn.CrossEntropyLoss
    vocab_size: *vocab_size
    lr_schedule_interval: "step"
    

data:
  # class_path: lm.data_module.ConcatDataModule
  class_path: lm.data_module.IncrementalLoadDataModule
  init_args:
    config:
      data_folder: ./data/
      # data_folder: &data_folder ../data/public/nlp/books1-small  # small test dataset
      # data_folder: ../data/public/nlp/books1
      samples_per_epoch: 5000
      batch_size: *batch_size
      seq_len: *seq_len
    tokenizer:
      class_path: lm.tokenizer.BytePairTokenizer
      init_args:
        config:
          token_folder_path: ./tokenizer/bpe-256
          # token_folder_path: ./tokenizer/books1-small-bpe-5000
          text_folder_path: ./data
          vocab_size: *vocab_size
          min_frequency: 10

      # class_path: lm.tokenizer.SentencePieceTokenizer
      # init_args:
      #   config:
      #     token_folder_path: ./tokenizer/spm-5000
      #     text_folder_path: *data_folder
      #     vocab_size: *vocab_size


optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0001
    betas: [0.9, 0.95]
    eps: 1e-5
    weight_decay: 0.1


lr_scheduler:
  class_path: lm.module.WarmupCosineDecayScheduler
  init_args:
    warmup_steps: 2000
    target_lr: 0.0001
    min_lr: 0.00001
    steps_in_cycle: 100000
    verbose: False