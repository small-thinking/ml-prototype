seed_everything: null
trainer:
  precision: 16-mixed
  max_epochs: 90
  logger:
    # - class_path: lightning.pytorch.loggers.TensorBoardLogger
    #   init_args:
    #     save_dir: ./lightning_logs
    #     name: 1-transformer-256-c128-b128-rmsn-pos
    - class_path: lightning.pytorch.loggers.WandbLogger
      init_args:
        project: dummy_model
        name: rms-lpe-b256-t384-d512-h8-l8-lr2e-04-dr0.05

    # - class_path: lm.model.CustomWandbLogger
    #   init_args:
    #     name: wandb
    #     project: dummy_model
    #     config:
    #       batch_size: &batch_size 256
    #       seq_len: &seq_len 384
    #       embed_dim: &embed_dim 512
    #       num_heads: &num_heads 4
    #       num_layers: &num_layers 2
    #       dropout_ratio: &dropout_ratio 0.05

  callbacks:
    - class_path: lm.model.DummyCallback
    - class_path: lm.model.TorchScriptCallback
      init_args:
        config:
          batch_size: &batch_size 256
          seq_len: &seq_len 384
          embed_dim: &embed_dim 512
          num_heads: &num_heads 8
          num_layers: &num_layers 8
          dropout_ratio: &dropout_ratio 0.05

          # batch_size: *batch_size
          # seq_len: *seq_len
          vocab_size: &vocab_size 215
          save_every_epoch: 5


model:
  class_path: lm.model.Seq2SeqLM
  init_args:
    model:
      class_path: lm.module.TransformerLM
      init_args:
        config:
          vocab_size: *vocab_size
          seq_len: *seq_len
          embed_dim: *embed_dim
          num_heads: *num_heads
          num_layers: *num_layers
          batch_first: true
          norm_first: true
          dropout_ratio: *dropout_ratio
          use_position_embedding: false
    loss:
      class_path: torch.nn.CrossEntropyLoss
    vocab_size: *vocab_size
    
data:
  class_path: lm.data_module.InMemoryDataModule
  init_args:
    config:
      data_path: ./data/
      samples_per_epoch: 50000
      batch_size: *batch_size
      seq_len: *seq_len
      token_file_path: ./data/tokens.json

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0002
    weight_decay: 0.01

lr_scheduler:
      class_path: torch.optim.lr_scheduler.StepLR
      init_args:
        step_size: 5
        gamma: 0.8
        verbose: True