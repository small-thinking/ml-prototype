seed_everything: 42
trainer:
  max_epochs: 10
  accelerator: mps
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: ./lightning_logs
      name: 1-transformer-256-c128-b128-rmsn-pos
  callbacks:
    class_path: lm.model.DummyCallback
    class_path: lm.model.TorchScriptCallback
    init_args:
      config:
        batch_size: &batch_size 128
        context_size: &context_size 128
        vocab_size: &vocab_size 65


model:
  class_path: lm.model.Seq2SeqLM
  init_args:
    model:
      class_path: lm.module.TransformerLM
      init_args:
        config:
          vocab_size: *vocab_size
          context_size: *context_size
          embed_dim: 256
          num_heads: 4
          num_layers: 1
          ffm_layer_sizes: [256, 256]
          batch_first: true
          norm_first: true
          dropout_ratio: 0.2
    loss:
      class_path: torch.nn.CrossEntropyLoss
    vocab_size: *vocab_size
    
data:
  class_path: lm.data_module.InMemoryDataModule
  init_args:
    config:
      data_path: ~/Downloads/test_data.txt
      samples_per_epoch: 100000
      batch_size: *batch_size
      context_size: *context_size

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.001
    weight_decay: 0.01

  
